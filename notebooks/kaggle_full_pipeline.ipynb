{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle End-to-End Pipeline\n",
        "\n",
        "This notebook rebuilds the entire angiogram segmentation project inside the Kaggle runtime, installs all dependencies, preprocesses the dataset, trains UNet++, UNet 3+, and TransUNet, and produces evaluation plots. Attach the angiogram dataset (folder name must remain `Database_134_Angiograms`) when creating the Kaggle notebook, enable GPU + Internet, and run the cells in order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Runtime checklist\n",
        "\n",
        "- Kaggle accelerator: GPU (T4/P100) with Internet ON\n",
        "- Data panel: add the angiogram dataset so `/kaggle/input/.../Database_134_Angiograms` exists\n",
        "- No code upload neededâ€”this notebook reconstructs the repo inside `/kaggle/working/angiogram-segmentation`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/kaggle/working/angiogram-segmentation\")\n",
        "PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = {\n",
        "    \"requirements.txt\": '''torch>=2.1.0\n",
        "torchvision>=0.16.0\n",
        "torchaudio>=2.1.0\n",
        "numpy==1.26.4\n",
        "pandas==2.2.2\n",
        "scikit-learn==1.6.1\n",
        "matplotlib==3.8.4\n",
        "seaborn==0.13.2\n",
        "albumentations==1.4.6\n",
        "opencv-python==4.9.0.80\n",
        "imageio==2.34.1\n",
        "Pillow==10.4.0\n",
        "tqdm==4.66.4\n",
        "''',\n",
        "    \"src/__init__.py\": '''\"\"\"\n",
        "Core package for coronary angiogram segmentation project.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "''',\n",
        "    \"src/utils/__init__.py\": '''\"\"\"\n",
        "Utility helpers for environment-specific behaviour.\n",
        "\"\"\"\n",
        "\n",
        "from .env import resolve_data_dir\n",
        "\n",
        "__all__ = [\"resolve_data_dir\"]\n",
        "\n",
        "''',\n",
        "    \"src/utils/env.py\": '''from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def resolve_data_dir(data_dir: str | Path) -> Path:\n",
        "    \"\"\"\n",
        "    Resolve the dataset directory, with Kaggle auto-discovery fallback.\n",
        "\n",
        "    Kaggle datasets are mounted read-only under /kaggle/input/<slug>/.\n",
        "    When running inside that environment we search for a folder that matches\n",
        "    the requested data directory name so users don't have to hard-code paths.\n",
        "    \"\"\"\n",
        "\n",
        "    path = Path(data_dir)\n",
        "    if path.exists():\n",
        "        return path\n",
        "\n",
        "    kaggle_root = Path(\"/kaggle/input\")\n",
        "    if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and kaggle_root.exists():\n",
        "        matches = sorted(kaggle_root.glob(f\"**/{path.name}\"))\n",
        "        if matches:\n",
        "            print(f\"[INFO] Resolved data directory to {matches[0]} (Kaggle input).\")\n",
        "            return matches[0]\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not locate data directory '{data_dir}'. \"\n",
        "        \"Pass --data_dir with an existing path or attach the dataset in Kaggle.\"\n",
        "    )\n",
        "''',\n",
        "    \"src/data/__init__.py\": '''\"\"\"\n",
        "Data loading and preprocessing utilities.\n",
        "\"\"\"\n",
        "\n",
        "from .dataset import AngiogramSegmentationDataset, load_image_mask_pairs\n",
        "\n",
        "__all__ = [\"AngiogramSegmentationDataset\", \"load_image_mask_pairs\"]\n",
        "\n",
        "\n",
        "''',\n",
        "    \"src/data/dataset.py\": '''\"\"\"\n",
        "Dataset utilities for coronary angiogram segmentation.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Sequence, Tuple\n",
        "\n",
        "import imageio.v2 as imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "DEFAULT_IMAGE_SUFFIX = \".pgm\"\n",
        "DEFAULT_MASK_SUFFIX = \"_gt.pgm\"\n",
        "\n",
        "\n",
        "def load_image_mask_pairs(\n",
        "    data_dir: Path | str,\n",
        "    image_suffix: str = DEFAULT_IMAGE_SUFFIX,\n",
        "    mask_suffix: str = DEFAULT_MASK_SUFFIX,\n",
        ") -> List[Tuple[Path, Path]]:\n",
        "    \"\"\"\n",
        "    Discover angiogram image and mask pairs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dir:\n",
        "        Root directory containing image and mask files.\n",
        "    image_suffix:\n",
        "        Expected suffix for image filenames. Default: ``.pgm``.\n",
        "    mask_suffix:\n",
        "        Expected suffix appended before the file extension for the mask files.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of tuple(Path, Path)\n",
        "        Sorted list of image and mask paths.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If no pairs are found within ``data_dir``.\n",
        "    \"\"\"\n",
        "\n",
        "    data_root = Path(data_dir)\n",
        "    if not data_root.exists():\n",
        "        raise FileNotFoundError(f\"Data directory not found: {data_root}\")\n",
        "\n",
        "    image_paths: List[Path] = sorted(data_root.glob(f\"*{image_suffix}\"))\n",
        "    pairs: List[Tuple[Path, Path]] = []\n",
        "    for image_path in image_paths:\n",
        "        stem = image_path.stem\n",
        "        if stem.endswith(\"_gt\"):\n",
        "            # Skip mask files we may have picked up because of the suffix glob\n",
        "            continue\n",
        "        mask_path = data_root / f\"{stem}{mask_suffix}\"\n",
        "        if mask_path.exists():\n",
        "            pairs.append((image_path, mask_path))\n",
        "\n",
        "    if not pairs:\n",
        "        raise FileNotFoundError(\n",
        "            f\"No image/mask pairs found in {data_root}. \"\n",
        "            \"Make sure the dataset is extracted correctly.\"\n",
        "        )\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DatasetSplits:\n",
        "    \"\"\"Container to hold absolute paths for dataset splits.\"\"\"\n",
        "\n",
        "    train: List[Tuple[Path, Path]]\n",
        "    val: List[Tuple[Path, Path]]\n",
        "    test: List[Tuple[Path, Path]]\n",
        "\n",
        "\n",
        "def create_splits(\n",
        "    pairs: Sequence[Tuple[Path, Path]],\n",
        "    val_size: float = 0.15,\n",
        "    test_size: float = 0.15,\n",
        "    seed: int = 42,\n",
        ") -> DatasetSplits:\n",
        "    \"\"\"\n",
        "    Split dataset pairs into train/validation/test partitions.\n",
        "\n",
        "    The test fraction is taken from the whole dataset, while the validation\n",
        "    fraction is computed relative to the remaining training portion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pairs:\n",
        "        Sequence of image/mask path tuples.\n",
        "    val_size:\n",
        "        Fraction of the entire dataset to reserve for validation.\n",
        "    test_size:\n",
        "        Fraction of the entire dataset to reserve for testing.\n",
        "    seed:\n",
        "        Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "\n",
        "    if not 0.0 < val_size < 1.0:\n",
        "        raise ValueError(\"val_size must be between 0 and 1\")\n",
        "    if not 0.0 < test_size < 1.0:\n",
        "        raise ValueError(\"test_size must be between 0 and 1\")\n",
        "    if val_size + test_size >= 1.0:\n",
        "        raise ValueError(\"val_size + test_size must be less than 1.0\")\n",
        "\n",
        "    train_pairs, test_pairs = train_test_split(\n",
        "        pairs, test_size=test_size, random_state=seed, shuffle=True\n",
        "    )\n",
        "    # Adjust validation size relative to the remaining examples.\n",
        "    relative_val_size = val_size / (1.0 - test_size)\n",
        "    train_pairs, val_pairs = train_test_split(\n",
        "        train_pairs,\n",
        "        test_size=relative_val_size,\n",
        "        random_state=seed,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    return DatasetSplits(\n",
        "        train=list(train_pairs),\n",
        "        val=list(val_pairs),\n",
        "        test=list(test_pairs),\n",
        "    )\n",
        "\n",
        "\n",
        "class AngiogramSegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Torch ``Dataset`` for coronary angiogram segmentation.\n",
        "\n",
        "    Each item returns a dict containing:\n",
        "\n",
        "    ``image`` (torch.float32 tensor): Normalised image of shape (1, H, W)\n",
        "    ``mask`` (torch.float32 tensor): Binary vessel mask of shape (1, H, W)\n",
        "    ``path`` (str): Path to the original image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        samples: Sequence[Tuple[Path, Path]],\n",
        "        transform: Optional[Callable] = None,\n",
        "        augment: Optional[Callable] = None,\n",
        "        normalize: bool = True,\n",
        "    ) -> None:\n",
        "        self.samples = list(samples)\n",
        "        if not self.samples:\n",
        "            raise ValueError(\"Dataset received an empty list of samples.\")\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
        "        image_path, mask_path = self.samples[index]\n",
        "        image = imageio.imread(image_path).astype(np.float32)\n",
        "        mask = imageio.imread(mask_path).astype(np.float32)\n",
        "\n",
        "        if self.normalize:\n",
        "            image = self._normalize_image(image)\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        # Expand dims for albumentations (expects HWC)\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "        if self.augment is not None:\n",
        "            aug_image = (image * 255.0).clip(0, 255).astype(np.uint8)\n",
        "            aug_mask = (mask * 255.0).clip(0, 255).astype(np.uint8)\n",
        "            augmented = self.augment(image=aug_image, mask=aug_mask)\n",
        "            image = augmented[\"image\"].astype(np.float32) / 255.0\n",
        "            mask = (augmented[\"mask\"] > 0).astype(np.float32)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image, mask = transformed[\"image\"], transformed[\"mask\"]\n",
        "\n",
        "        image = np.transpose(image, (2, 0, 1))  # to CHW\n",
        "        mask = np.transpose(mask, (2, 0, 1))\n",
        "\n",
        "        return {\n",
        "            \"image\": torch.from_numpy(image).float(),\n",
        "            \"mask\": torch.from_numpy(mask).float(),\n",
        "            \"path\": str(image_path),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_image(image: np.ndarray) -> np.ndarray:\n",
        "        min_val = image.min()\n",
        "        max_val = image.max()\n",
        "        if max_val - min_val < 1e-6:\n",
        "            return np.zeros_like(image, dtype=np.float32)\n",
        "        return (image - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "def default_resize_transform(size: Tuple[int, int]) -> Callable:\n",
        "    \"\"\"Return an Albumentations resize transform to the given ``(height, width)``.\"\"\"\n",
        "    import albumentations as A\n",
        "\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(height=size[0], width=size[1], interpolation=1),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def default_augmentation_pipeline() -> Callable:\n",
        "    \"\"\"\n",
        "    Construct a default albumentations augmentation pipeline.\n",
        "\n",
        "    Includes rotations, flips, elastic transforms, and contrast adjustments.\n",
        "    \"\"\"\n",
        "\n",
        "    import albumentations as A\n",
        "\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.2),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.0625,\n",
        "                scale_limit=0.1,\n",
        "                rotate_limit=25,\n",
        "                border_mode=0,\n",
        "                p=0.75,\n",
        "            ),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.CLAHE(p=0.2),\n",
        "            A.ElasticTransform(alpha=120, sigma=120 * 0.07, alpha_affine=10, p=0.2),\n",
        "            A.GaussNoise(var_limit=(0.0, 0.001), p=0.2),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def create_datasets(\n",
        "    data_dir: Path | str,\n",
        "    image_size: Tuple[int, int] = (512, 512),\n",
        "    val_size: float = 0.15,\n",
        "    test_size: float = 0.15,\n",
        "    seed: int = 42,\n",
        "    augment: bool = True,\n",
        ") -> Tuple[AngiogramSegmentationDataset, AngiogramSegmentationDataset, AngiogramSegmentationDataset]:\n",
        "    \"\"\"\n",
        "    High level helper returning train/val/test ``Dataset`` objects.\n",
        "    \"\"\"\n",
        "\n",
        "    pairs = load_image_mask_pairs(data_dir)\n",
        "    splits = create_splits(pairs, val_size=val_size, test_size=test_size, seed=seed)\n",
        "    resize_transform = default_resize_transform(image_size)\n",
        "    train_aug = default_augmentation_pipeline() if augment else None\n",
        "    train_dataset = AngiogramSegmentationDataset(\n",
        "        splits.train,\n",
        "        transform=resize_transform,\n",
        "        augment=train_aug,\n",
        "    )\n",
        "    val_dataset = AngiogramSegmentationDataset(\n",
        "        splits.val,\n",
        "        transform=resize_transform,\n",
        "        augment=None,\n",
        "    )\n",
        "    test_dataset = AngiogramSegmentationDataset(\n",
        "        splits.test,\n",
        "        transform=resize_transform,\n",
        "        augment=None,\n",
        "    )\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def create_dataloaders(\n",
        "    datasets: Tuple[Dataset, Dataset, Dataset],\n",
        "    batch_size: int = 4,\n",
        "    num_workers: int = 0,\n",
        ") -> Tuple[torch.utils.data.DataLoader, ...]:\n",
        "    \"\"\"\n",
        "    Instantiate ``DataLoader`` objects for the provided datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = datasets\n",
        "    return (\n",
        "        torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
        "        ),\n",
        "        torch.utils.data.DataLoader(\n",
        "            val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "        ),\n",
        "        torch.utils.data.DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "        ),\n",
        "    )\n",
        "\n",
        "''',\n",
        "    \"src/preprocessing.py\": '''\"\"\"\n",
        "Preprocessing and exploratory utilities for angiogram segmentation dataset.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Sequence, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from src.data.dataset import (\n",
        "    AngiogramSegmentationDataset,\n",
        "    create_dataloaders,\n",
        "    create_datasets,\n",
        "    create_splits,\n",
        "    load_image_mask_pairs,\n",
        ")\n",
        "\n",
        "\n",
        "def seed_everything(seed: int = 42) -> None:\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility across ``random``, ``numpy`` and ``torch``.\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def describe_dataset(pairs: Sequence[Tuple[Path, Path]]) -> dict:\n",
        "    \"\"\"\n",
        "    Compute simple dataset statistics.\n",
        "\n",
        "    Returns a dictionary containing total counts and basic image statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    import imageio.v2 as imageio\n",
        "\n",
        "    stats = {\n",
        "        \"num_samples\": len(pairs),\n",
        "        \"image_mean\": 0.0,\n",
        "        \"image_std\": 0.0,\n",
        "        \"mask_mean\": 0.0,\n",
        "    }\n",
        "    if not pairs:\n",
        "        return stats\n",
        "\n",
        "    means: List[float] = []\n",
        "    stds: List[float] = []\n",
        "    mask_means: List[float] = []\n",
        "    for image_path, mask_path in pairs:\n",
        "        image = imageio.imread(image_path).astype(np.float32)\n",
        "        mask = imageio.imread(mask_path).astype(np.float32)\n",
        "        means.append(image.mean())\n",
        "        stds.append(image.std())\n",
        "        mask_means.append(mask.mean() / 255.0)\n",
        "\n",
        "    stats[\"image_mean\"] = float(np.mean(means))\n",
        "    stats[\"image_std\"] = float(np.mean(stds))\n",
        "    stats[\"mask_mean\"] = float(np.mean(mask_means))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def visualize_samples(\n",
        "    dataset: AngiogramSegmentationDataset,\n",
        "    num_samples: int = 4,\n",
        "    figsize: Tuple[int, int] = (12, 6),\n",
        "    save_path: Path | None = None,\n",
        "    show: bool = True,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot ``num_samples`` random images and masks from the dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset:\n",
        "        Dataset to sample from.\n",
        "    num_samples:\n",
        "        Number of examples to plot.\n",
        "    figsize:\n",
        "        Matplotlib figure size.\n",
        "    save_path:\n",
        "        If provided, save the figure to this path instead of (or in addition to)\n",
        "        showing it interactively.\n",
        "    show:\n",
        "        Whether to call ``plt.show()``. Automatically disabled when running in\n",
        "        non-interactive environments by setting ``show=False``.\n",
        "    \"\"\"\n",
        "\n",
        "    indices = random.sample(range(len(dataset)), k=min(num_samples, len(dataset)))\n",
        "    fig, axes = plt.subplots(len(indices), 2, figsize=figsize)\n",
        "    if len(indices) == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "    for row, idx in zip(axes, indices):\n",
        "        sample = dataset[idx]\n",
        "        image = sample[\"image\"].squeeze().numpy()\n",
        "        mask = sample[\"mask\"].squeeze().numpy()\n",
        "        row[0].imshow(image, cmap=\"gray\")\n",
        "        row[0].set_title(\"Image\")\n",
        "        row[0].axis(\"off\")\n",
        "        row[1].imshow(image, cmap=\"gray\")\n",
        "        row[1].imshow(mask, cmap=\"jet\", alpha=0.4)\n",
        "        row[1].set_title(\"Mask Overlay\")\n",
        "        row[1].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    if save_path is not None:\n",
        "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"seed_everything\",\n",
        "    \"describe_dataset\",\n",
        "    \"visualize_samples\",\n",
        "    \"load_image_mask_pairs\",\n",
        "    \"create_splits\",\n",
        "    \"create_datasets\",\n",
        "    \"create_dataloaders\",\n",
        "]\n",
        "\n",
        "''',\n",
        "    \"src/metrics.py\": '''\"\"\"\n",
        "Losses and metrics for segmentation models.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Callable, Dict, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def dice_coefficient(\n",
        "    preds: torch.Tensor, targets: torch.Tensor, epsilon: float = 1e-6\n",
        ") -> torch.Tensor:\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = preds.view(preds.size(0), -1)\n",
        "    targets = targets.view(targets.size(0), -1)\n",
        "    intersection = (preds * targets).sum(dim=1)\n",
        "    union = preds.sum(dim=1) + targets.sum(dim=1)\n",
        "    dice = (2 * intersection + epsilon) / (union + epsilon)\n",
        "    return dice.mean()\n",
        "\n",
        "\n",
        "def iou_score(\n",
        "    preds: torch.Tensor, targets: torch.Tensor, epsilon: float = 1e-6\n",
        ") -> torch.Tensor:\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = preds.view(preds.size(0), -1)\n",
        "    targets = targets.view(targets.size(0), -1)\n",
        "    intersection = (preds * targets).sum(dim=1)\n",
        "    total = preds.sum(dim=1) + targets.sum(dim=1)\n",
        "    union = total - intersection\n",
        "    iou = (intersection + epsilon) / (union + epsilon)\n",
        "    return iou.mean()\n",
        "\n",
        "\n",
        "def precision_recall(\n",
        "    preds: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5, epsilon: float = 1e-6\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = (preds > threshold).float()\n",
        "\n",
        "    tp = (preds * targets).sum(dim=[1, 2, 3])\n",
        "    fp = (preds * (1 - targets)).sum(dim=[1, 2, 3])\n",
        "    fn = ((1 - preds) * targets).sum(dim=[1, 2, 3])\n",
        "\n",
        "    precision = (tp + epsilon) / (tp + fp + epsilon)\n",
        "    recall = (tp + epsilon) / (tp + fn + epsilon)\n",
        "    return precision.mean(), recall.mean()\n",
        "\n",
        "\n",
        "def dice_loss(\n",
        "    preds: torch.Tensor, targets: torch.Tensor, smooth: float = 1e-6\n",
        ") -> torch.Tensor:\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds_flat = preds.contiguous().view(preds.size(0), -1)\n",
        "    targets_flat = targets.contiguous().view(targets.size(0), -1)\n",
        "    intersection = (preds_flat * targets_flat).sum(dim=1)\n",
        "    union = preds_flat.sum(dim=1) + targets_flat.sum(dim=1)\n",
        "    loss = 1 - ((2 * intersection + smooth) / (union + smooth))\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def bce_dice_loss(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "    bce = F.binary_cross_entropy_with_logits(preds, targets)\n",
        "    d_loss = dice_loss(preds, targets)\n",
        "    return bce + d_loss\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    preds: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.8,\n",
        "    gamma: float = 2.0,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    preds_prob = torch.sigmoid(preds)\n",
        "    ce_loss = F.binary_cross_entropy(preds_prob, targets, reduction=\"none\")\n",
        "    pt = torch.where(targets == 1, preds_prob, 1 - preds_prob)\n",
        "    loss = ce_loss * ((1 - pt) ** gamma)\n",
        "    if alpha >= 0:\n",
        "        alpha_factor = torch.where(targets == 1, alpha, 1 - alpha)\n",
        "        loss = alpha_factor * loss\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    if reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "MetricFn = Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    preds: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    dice = dice_coefficient(preds, targets)\n",
        "    iou = iou_score(preds, targets)\n",
        "    precision, recall = precision_recall(preds, targets)\n",
        "    return {\n",
        "        \"dice\": dice,\n",
        "        \"iou\": iou,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "    }\n",
        "\n",
        "''',\n",
        "    \"src/models/__init__.py\": '''\"\"\"\n",
        "Model architectures for coronary artery segmentation.\n",
        "\"\"\"\n",
        "\n",
        "from .unetpp import UNetPlusPlus\n",
        "from .unet3plus import UNet3Plus\n",
        "from .transunet import TransUNet\n",
        "\n",
        "__all__ = [\"UNetPlusPlus\", \"UNet3Plus\", \"TransUNet\"]\n",
        "\n",
        "\n",
        "''',\n",
        "    \"src/models/unetpp.py\": '''\"\"\"\n",
        "UNet++ implementation for coronary angiogram segmentation.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, dropout: float = 0.0) -> None:\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        if dropout > 0:\n",
        "            layers.insert(3, nn.Dropout2d(dropout))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # noqa: D401\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class UNetPlusPlus(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of UNet++ (Nested U-Net) with optional deep supervision.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int = 1,\n",
        "        out_channels: int = 1,\n",
        "        filters: Sequence[int] = (32, 64, 128, 256, 512),\n",
        "        deep_supervision: bool = False,\n",
        "        up_mode: str = \"bilinear\",\n",
        "        dropout: float = 0.0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.deep_supervision = deep_supervision\n",
        "\n",
        "        self.conv0_0 = ConvBlock(in_channels, filters[0], dropout=dropout)\n",
        "        self.conv1_0 = ConvBlock(filters[0], filters[1], dropout=dropout)\n",
        "        self.conv2_0 = ConvBlock(filters[1], filters[2], dropout=dropout)\n",
        "        self.conv3_0 = ConvBlock(filters[2], filters[3], dropout=dropout)\n",
        "        self.conv4_0 = ConvBlock(filters[3], filters[4], dropout=dropout)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv0_1 = ConvBlock(filters[0] + filters[1], filters[0])\n",
        "        self.conv1_1 = ConvBlock(filters[1] + filters[2], filters[1])\n",
        "        self.conv2_1 = ConvBlock(filters[2] + filters[3], filters[2])\n",
        "        self.conv3_1 = ConvBlock(filters[3] + filters[4], filters[3])\n",
        "\n",
        "        self.conv0_2 = ConvBlock(filters[0] * 2 + filters[1], filters[0])\n",
        "        self.conv1_2 = ConvBlock(filters[1] * 2 + filters[2], filters[1])\n",
        "        self.conv2_2 = ConvBlock(filters[2] * 2 + filters[3], filters[2])\n",
        "\n",
        "        self.conv0_3 = ConvBlock(filters[0] * 3 + filters[1], filters[0])\n",
        "        self.conv1_3 = ConvBlock(filters[1] * 3 + filters[2], filters[1])\n",
        "\n",
        "        self.conv0_4 = ConvBlock(filters[0] * 4 + filters[1], filters[0])\n",
        "\n",
        "        if deep_supervision:\n",
        "            self.final1 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "            self.final2 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "            self.final3 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "            self.final4 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.final = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor | Tuple[torch.Tensor, ...]:\n",
        "        x0_0 = self.conv0_0(x)  # down path\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self._upsample(x1_0, x0_0)], dim=1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self._upsample(x2_0, x1_0)], dim=1))\n",
        "        x0_2 = self.conv0_2(\n",
        "            torch.cat([x0_0, x0_1, self._upsample(x1_1, x0_0)], dim=1)\n",
        "        )\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self._upsample(x3_0, x2_0)], dim=1))\n",
        "        x1_2 = self.conv1_2(\n",
        "            torch.cat([x1_0, x1_1, self._upsample(x2_1, x1_0)], dim=1)\n",
        "        )\n",
        "        x0_3 = self.conv0_3(\n",
        "            torch.cat([x0_0, x0_1, x0_2, self._upsample(x1_2, x0_0)], dim=1)\n",
        "        )\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self._upsample(x4_0, x3_0)], dim=1))\n",
        "        x2_2 = self.conv2_2(\n",
        "            torch.cat([x2_0, x2_1, self._upsample(x3_1, x2_0)], dim=1)\n",
        "        )\n",
        "        x1_3 = self.conv1_3(\n",
        "            torch.cat([x1_0, x1_1, x1_2, self._upsample(x2_2, x1_0)], dim=1)\n",
        "        )\n",
        "        x0_4 = self.conv0_4(\n",
        "            torch.cat([x0_0, x0_1, x0_2, x0_3, self._upsample(x1_3, x0_0)], dim=1)\n",
        "        )\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            outputs = (\n",
        "                self.final1(x0_1),\n",
        "                self.final2(x0_2),\n",
        "                self.final3(x0_3),\n",
        "                self.final4(x0_4),\n",
        "            )\n",
        "            return outputs\n",
        "        return self.final(x0_4)\n",
        "\n",
        "    @staticmethod\n",
        "    def _upsample(source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        return F.interpolate(\n",
        "            source, size=target.shape[2:], mode=\"bilinear\", align_corners=True\n",
        "        )\n",
        "\n",
        "''',\n",
        "    \"src/models/unet3plus.py\": '''\"\"\"\n",
        "UNet 3+ implementation for coronary angiogram segmentation.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # noqa: D401\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "def _resize(\n",
        "    tensor: torch.Tensor, target_shape: torch.Size, mode: str = \"bilinear\"\n",
        ") -> torch.Tensor:\n",
        "    if tensor.shape[-2:] == target_shape[-2:]:\n",
        "        return tensor\n",
        "    return F.interpolate(tensor, size=target_shape[-2:], mode=mode, align_corners=True)\n",
        "\n",
        "\n",
        "class UNet3Plus(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet 3+ implementation with deep supervision support.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int = 1,\n",
        "        out_channels: int = 1,\n",
        "        filters: Sequence[int] = (32, 64, 128, 256, 512),\n",
        "        cat_channels: int = 32,\n",
        "        deep_supervision: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if len(filters) != 5:\n",
        "            raise ValueError(\"UNet3Plus expects five filter values for the encoder.\")\n",
        "        self.deep_supervision = deep_supervision\n",
        "        self.cat_channels = cat_channels\n",
        "\n",
        "        self.encoder1 = ConvBlock(in_channels, filters[0])\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.encoder2 = ConvBlock(filters[0], filters[1])\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.encoder3 = ConvBlock(filters[1], filters[2])\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.encoder4 = ConvBlock(filters[2], filters[3])\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "        self.encoder5 = ConvBlock(filters[3], filters[4])\n",
        "\n",
        "        self.h1_d4 = self._make_stage(filters[0], filters[0])\n",
        "        self.h2_d4 = self._make_stage(filters[1], filters[0])\n",
        "        self.h3_d4 = self._make_stage(filters[2], filters[0])\n",
        "        self.h4_d4 = self._make_stage(filters[3], filters[0])\n",
        "        self.h5_d4 = self._make_stage(filters[4], filters[0])\n",
        "\n",
        "        self.h1_d3 = self._make_stage(filters[0], filters[1])\n",
        "        self.h2_d3 = self._make_stage(filters[1], filters[1])\n",
        "        self.h3_d3 = self._make_stage(filters[2], filters[1])\n",
        "        self.h4_d3 = self._make_stage(filters[3], filters[1])\n",
        "        self.h5_d3 = self._make_stage(filters[4], filters[1])\n",
        "\n",
        "        self.h1_d2 = self._make_stage(filters[0], filters[2])\n",
        "        self.h2_d2 = self._make_stage(filters[1], filters[2])\n",
        "        self.h3_d2 = self._make_stage(filters[2], filters[2])\n",
        "        self.h4_d2 = self._make_stage(filters[3], filters[2])\n",
        "        self.h5_d2 = self._make_stage(filters[4], filters[2])\n",
        "\n",
        "        self.h1_d1 = self._make_stage(filters[0], filters[3])\n",
        "        self.h2_d1 = self._make_stage(filters[1], filters[3])\n",
        "        self.h3_d1 = self._make_stage(filters[2], filters[3])\n",
        "        self.h4_d1 = self._make_stage(filters[3], filters[3])\n",
        "        self.h5_d1 = self._make_stage(filters[4], filters[3])\n",
        "\n",
        "        self.h1_d0 = self._make_stage(filters[0], filters[4])\n",
        "        self.h2_d0 = self._make_stage(filters[1], filters[4])\n",
        "        self.h3_d0 = self._make_stage(filters[2], filters[4])\n",
        "        self.h4_d0 = self._make_stage(filters[3], filters[4])\n",
        "        self.h5_d0 = self._make_stage(filters[4], filters[4])\n",
        "\n",
        "        concat_channels_1 = cat_channels * 5\n",
        "        self.conv_d4 = ConvBlock(concat_channels_1, filters[3])\n",
        "        self.conv_d3 = ConvBlock(concat_channels_1, filters[2])\n",
        "        self.conv_d2 = ConvBlock(concat_channels_1, filters[1])\n",
        "        self.conv_d1 = ConvBlock(concat_channels_1, filters[0])\n",
        "\n",
        "        self.final = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "\n",
        "        if deep_supervision:\n",
        "            self.ds_out1 = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "            self.ds_out2 = nn.Conv2d(filters[1], out_channels, kernel_size=1)\n",
        "            self.ds_out3 = nn.Conv2d(filters[2], out_channels, kernel_size=1)\n",
        "            self.ds_out4 = nn.Conv2d(filters[3], out_channels, kernel_size=1)\n",
        "\n",
        "    def _make_stage(self, in_channels: int, target_filters: int) -> nn.Sequential:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.cat_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.cat_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor | tuple[torch.Tensor, ...]:\n",
        "        h1 = self.encoder1(x)\n",
        "        h2 = self.encoder2(self.pool1(h1))\n",
        "        h3 = self.encoder3(self.pool2(h2))\n",
        "        h4 = self.encoder4(self.pool3(h3))\n",
        "        h5 = self.encoder5(self.pool4(h4))\n",
        "\n",
        "        # Stage 4\n",
        "        size4 = h4.size()\n",
        "        d4_1 = torch.cat(\n",
        "            [\n",
        "                self.h1_d4(_resize(h1, size4)),\n",
        "                self.h2_d4(_resize(h2, size4)),\n",
        "                self.h3_d4(_resize(h3, size4)),\n",
        "                self.h4_d4(h4),\n",
        "                self.h5_d4(F.interpolate(h5, size=size4[-2:], mode=\"bilinear\", align_corners=True)),\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        d4 = self.conv_d4(d4_1)\n",
        "\n",
        "        # Stage 3\n",
        "        size3 = h3.size()\n",
        "        d3_1 = torch.cat(\n",
        "            [\n",
        "                self.h1_d3(_resize(h1, size3)),\n",
        "                self.h2_d3(_resize(h2, size3)),\n",
        "                self.h3_d3(h3),\n",
        "                self.h4_d3(_resize(h4, size3)),\n",
        "                self.h5_d3(_resize(h5, size3)),\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        d3 = self.conv_d3(d3_1)\n",
        "\n",
        "        # Stage 2\n",
        "        size2 = h2.size()\n",
        "        d2_1 = torch.cat(\n",
        "            [\n",
        "                self.h1_d2(_resize(h1, size2)),\n",
        "                self.h2_d2(h2),\n",
        "                self.h3_d2(_resize(h3, size2)),\n",
        "                self.h4_d2(_resize(h4, size2)),\n",
        "                self.h5_d2(_resize(h5, size2)),\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        d2 = self.conv_d2(d2_1)\n",
        "\n",
        "        # Stage 1\n",
        "        size1 = h1.size()\n",
        "        d1_1 = torch.cat(\n",
        "            [\n",
        "                self.h1_d1(h1),\n",
        "                self.h2_d1(_resize(h2, size1)),\n",
        "                self.h3_d1(_resize(h3, size1)),\n",
        "                self.h4_d1(_resize(h4, size1)),\n",
        "                self.h5_d1(_resize(h5, size1)),\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        d1 = self.conv_d1(d1_1)\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            ds1 = self.ds_out1(d1)\n",
        "            ds2 = self.ds_out2(d2)\n",
        "            ds3 = self.ds_out3(d3)\n",
        "            ds4 = self.ds_out4(d4)\n",
        "            return ds1, ds2, ds3, ds4\n",
        "        return self.final(d1)\n",
        "\n",
        "''',\n",
        "    \"src/models/transunet.py\": '''\"\"\"\n",
        "Simplified TransUNet implementation combining CNN encoder and Transformer bottleneck.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # noqa: D401\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        embed_dim: int,\n",
        "        patch_size: int,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)  # B, N, C\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBottleneck(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        depth: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.0,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # noqa: D401\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, skip_channels: int, out_channels: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv = ConvBlock(in_channels + skip_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=True)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class TransUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based U-Net variant for angiogram segmentation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int = 1,\n",
        "        out_channels: int = 1,\n",
        "        img_size: Tuple[int, int] = (512, 512),\n",
        "        filters: Sequence[int] = (32, 64, 128, 256),\n",
        "        embed_dim: int = 256,\n",
        "        transformer_depth: int = 4,\n",
        "        num_heads: int = 8,\n",
        "        patch_size: int = 16,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        height, width = img_size\n",
        "        if height % patch_size != 0 or width % patch_size != 0:\n",
        "            raise ValueError(\"img_size must be divisible by patch_size.\")\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.enc1 = ConvBlock(in_channels, filters[0])\n",
        "        self.enc2 = ConvBlock(filters[0], filters[1])\n",
        "        self.enc3 = ConvBlock(filters[1], filters[2])\n",
        "        self.enc4 = ConvBlock(filters[2], filters[3])\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        downscale = 2 ** (len(filters) - 1)\n",
        "        bottleneck_hw = (height // downscale, width // downscale)\n",
        "        patch_kernel = max(1, patch_size // downscale)\n",
        "        if bottleneck_hw[0] % patch_kernel != 0 or bottleneck_hw[1] % patch_kernel != 0:\n",
        "            raise ValueError(\n",
        "                \"Incompatible patch/kernel configuration. \"\n",
        "                \"Ensure patch_size leads to integer number of tokens.\"\n",
        "            )\n",
        "        token_hw = (bottleneck_hw[0] // patch_kernel, bottleneck_hw[1] // patch_kernel)\n",
        "        self.patch_embed = PatchEmbedding(filters[3], embed_dim, patch_size=patch_kernel)\n",
        "\n",
        "        num_patches = token_hw[0] * token_hw[1]\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        self.transformer = TransformerBottleneck(\n",
        "            embed_dim=embed_dim,\n",
        "            depth=transformer_depth,\n",
        "            num_heads=num_heads,\n",
        "        )\n",
        "        self.proj_back = nn.Conv2d(embed_dim, filters[3], kernel_size=1)\n",
        "        self._bottleneck_hw = bottleneck_hw\n",
        "        self._token_hw = token_hw\n",
        "\n",
        "        self.up1 = UpBlock(filters[3], filters[2], filters[2])\n",
        "        self.up2 = UpBlock(filters[2], filters[1], filters[1])\n",
        "        self.up3 = UpBlock(filters[1], filters[0], filters[0])\n",
        "\n",
        "        self.final = nn.Conv2d(filters[0], out_channels, kernel_size=1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h1 = self.enc1(x)\n",
        "        h2 = self.enc2(self.pool(h1))\n",
        "        h3 = self.enc3(self.pool(h2))\n",
        "        h4 = self.enc4(self.pool(h3))\n",
        "\n",
        "        tokens = self.patch_embed(h4)  # B, N, C\n",
        "        tokens = tokens + self.pos_embed[:, : tokens.size(1), :]\n",
        "        tokens = self.transformer(tokens)\n",
        "\n",
        "        b, _, c = tokens.shape\n",
        "        tokens = tokens.transpose(1, 2).contiguous()\n",
        "        tokens = tokens.view(b, c, self._token_hw[0], self._token_hw[1])\n",
        "        tokens = F.interpolate(\n",
        "            tokens,\n",
        "            size=self._bottleneck_hw,\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=True,\n",
        "        )\n",
        "        bottleneck = self.proj_back(tokens)\n",
        "\n",
        "        d3 = self.up1(bottleneck, h3)\n",
        "        d2 = self.up2(d3, h2)\n",
        "        d1 = self.up3(d2, h1)\n",
        "        return self.final(d1)\n",
        "\n",
        "''',\n",
        "    \"src/preprocess_dataset.py\": '''\"\"\"\n",
        "Command-line preprocessing utility to create persistent dataset splits\n",
        "and optionally export resized numpy arrays.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Tuple\n",
        "\n",
        "import imageio.v2 as imageio\n",
        "import numpy as np\n",
        "\n",
        "from src.data.dataset import (\n",
        "    AngiogramSegmentationDataset,\n",
        "    create_splits,\n",
        "    default_resize_transform,\n",
        "    load_image_mask_pairs,\n",
        ")\n",
        "from src.preprocessing import seed_everything\n",
        "from src.utils.env import resolve_data_dir\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Generate train/val/test splits and optionally export preprocessed arrays.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        type=Path,\n",
        "        default=Path(\"Database_134_Angiograms\"),\n",
        "        help=\"Directory containing angiogram *.pgm files and *_gt.pgm masks.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        type=Path,\n",
        "        default=Path(\"results/preprocessed\"),\n",
        "        help=\"Directory where splits, manifests, and exported arrays will be stored.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--image_size\",\n",
        "        type=int,\n",
        "        nargs=2,\n",
        "        default=(512, 512),\n",
        "        metavar=(\"HEIGHT\", \"WIDTH\"),\n",
        "        help=\"Target height and width for resizing images/masks.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--val_size\",\n",
        "        type=float,\n",
        "        default=0.15,\n",
        "        help=\"Fraction of data reserved for validation.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test_size\",\n",
        "        type=float,\n",
        "        default=0.15,\n",
        "        help=\"Fraction of data reserved for testing.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\",\n",
        "        type=int,\n",
        "        default=42,\n",
        "        help=\"Random seed controlling the split.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--export\",\n",
        "        action=\"store_true\",\n",
        "        help=\"If set, export resized numpy arrays for each split.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--format\",\n",
        "        choices=(\"npy\", \"npz\"),\n",
        "        default=\"npy\",\n",
        "        help=\"File format used when exporting arrays.\",\n",
        "    )\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def normalize(image: np.ndarray) -> np.ndarray:\n",
        "    min_val = image.min()\n",
        "    max_val = image.max()\n",
        "    if max_val - min_val < 1e-6:\n",
        "        return np.zeros_like(image, dtype=np.float32)\n",
        "    return (image - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "def export_split(\n",
        "    split_name: str,\n",
        "    pairs: Iterable[Tuple[Path, Path]],\n",
        "    resize_transform,\n",
        "    output_dir: Path,\n",
        "    file_format: str,\n",
        ") -> List[Dict[str, str]]:\n",
        "    manifest: List[Dict[str, str]] = []\n",
        "    images_dir = output_dir / split_name / \"images\"\n",
        "    masks_dir = output_dir / split_name / \"masks\"\n",
        "    images_dir.mkdir(parents=True, exist_ok=True)\n",
        "    masks_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for image_path, mask_path in pairs:\n",
        "        image = imageio.imread(image_path).astype(np.float32)\n",
        "        mask = imageio.imread(mask_path).astype(np.float32)\n",
        "\n",
        "        image = normalize(image)\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        if resize_transform is not None:\n",
        "            transformed = resize_transform(image=image, mask=mask)\n",
        "            image, mask = transformed[\"image\"], transformed[\"mask\"]\n",
        "\n",
        "        image = np.transpose(image, (2, 0, 1))  # to CHW\n",
        "        mask = np.transpose(mask, (2, 0, 1))\n",
        "\n",
        "        stem = image_path.stem.replace(\"_gt\", \"\")\n",
        "        image_file = images_dir / f\"{stem}.{file_format}\"\n",
        "        mask_file = masks_dir / f\"{stem}.{file_format}\"\n",
        "\n",
        "        if file_format == \"npy\":\n",
        "            np.save(image_file, image, allow_pickle=False)\n",
        "            np.save(mask_file, mask, allow_pickle=False)\n",
        "        else:\n",
        "            np.savez_compressed(image_file, image=image)\n",
        "            np.savez_compressed(mask_file, mask=mask)\n",
        "\n",
        "        manifest.append(\n",
        "            {\n",
        "                \"stem\": stem,\n",
        "                \"original_image\": str(image_path),\n",
        "                \"original_mask\": str(mask_path),\n",
        "                \"image_file\": str(image_file),\n",
        "                \"mask_file\": str(mask_file),\n",
        "            }\n",
        "        )\n",
        "    return manifest\n",
        "\n",
        "\n",
        "def save_manifest(manifest: Dict[str, List[Dict[str, str]]], output_dir: Path) -> Path:\n",
        "    manifest_path = output_dir / \"manifest.json\"\n",
        "    with manifest_path.open(\"w\") as f:\n",
        "        json.dump(manifest, f, indent=2)\n",
        "    return manifest_path\n",
        "\n",
        "\n",
        "def save_splits(splits: Dict[str, List[Tuple[Path, Path]]], output_dir: Path) -> Path:\n",
        "    serializable = {\n",
        "        split: [\n",
        "            {\"image\": str(image_path), \"mask\": str(mask_path)}\n",
        "            for image_path, mask_path in pairs\n",
        "        ]\n",
        "        for split, pairs in splits.items()\n",
        "    }\n",
        "    splits_path = output_dir / \"splits.json\"\n",
        "    with splits_path.open(\"w\") as f:\n",
        "        json.dump(serializable, f, indent=2)\n",
        "    return splits_path\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    seed_everything(args.seed)\n",
        "    output_dir = args.output_dir\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    data_dir = resolve_data_dir(args.data_dir)\n",
        "    print(f\"Collecting image/mask pairs from {data_dir}...\")\n",
        "    pairs = load_image_mask_pairs(data_dir)\n",
        "    splits = create_splits(\n",
        "        pairs,\n",
        "        val_size=args.val_size,\n",
        "        test_size=args.test_size,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "\n",
        "    splits_dict = {\n",
        "        \"train\": splits.train,\n",
        "        \"val\": splits.val,\n",
        "        \"test\": splits.test,\n",
        "    }\n",
        "    splits_path = save_splits(splits_dict, output_dir)\n",
        "    print(f\"Saved split definitions to {splits_path}\")\n",
        "\n",
        "    if not args.export:\n",
        "        print(\"Export flag not set; skipping array export.\")\n",
        "        return\n",
        "\n",
        "    resize_transform = default_resize_transform(tuple(args.image_size))\n",
        "    manifest: Dict[str, List[Dict[str, str]]] = {}\n",
        "    for split_name, split_pairs in splits_dict.items():\n",
        "        print(f\"Exporting {split_name} split with {len(split_pairs)} samples...\")\n",
        "        manifest[split_name] = export_split(\n",
        "            split_name,\n",
        "            split_pairs,\n",
        "            resize_transform,\n",
        "            output_dir=output_dir,\n",
        "            file_format=args.format,\n",
        "        )\n",
        "\n",
        "    manifest_path = save_manifest(manifest, output_dir)\n",
        "    print(f\"Saved export manifest to {manifest_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "''',\n",
        "    \"src/train.py\": '''\"\"\"\n",
        "Training script for coronary angiogram segmentation models.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda import amp\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.metrics import bce_dice_loss, compute_metrics, dice_loss, focal_loss\n",
        "from src.models import TransUNet, UNet3Plus, UNetPlusPlus\n",
        "from src.preprocessing import create_dataloaders, create_datasets, seed_everything\n",
        "from src.utils.env import resolve_data_dir\n",
        "\n",
        "\n",
        "LOSS_MAP = {\n",
        "    \"dice\": dice_loss,\n",
        "    \"bce_dice\": bce_dice_loss,\n",
        "    \"focal\": focal_loss,\n",
        "}\n",
        "\n",
        "MODEL_MAP = {\n",
        "    \"unetpp\": UNetPlusPlus,\n",
        "    \"unet3plus\": UNet3Plus,\n",
        "    \"transunet\": TransUNet,\n",
        "}\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=\"Train segmentation models on angiograms.\")\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        type=str,\n",
        "        default=\"Database_134_Angiograms\",\n",
        "        help=\"Directory containing angiogram images and masks.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        type=str,\n",
        "        default=\"results\",\n",
        "        help=\"Directory to store checkpoints and metrics.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--image_size\",\n",
        "        type=int,\n",
        "        nargs=2,\n",
        "        default=(512, 512),\n",
        "        help=\"Resize all images to this size (height width).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--models\",\n",
        "        type=str,\n",
        "        nargs=\"+\",\n",
        "        default=[\"unetpp\", \"unet3plus\", \"transunet\"],\n",
        "        choices=list(MODEL_MAP.keys()),\n",
        "        help=\"Models to train.\",\n",
        "    )\n",
        "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=1e-5)\n",
        "    parser.add_argument(\"--num_workers\", type=int, default=0)\n",
        "    parser.add_argument(\"--loss\", type=str, default=\"bce_dice\", choices=list(LOSS_MAP.keys()))\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    parser.add_argument(\"--patience\", type=int, default=10, help=\"Early stopping patience.\")\n",
        "    parser.add_argument(\"--amp\", action=\"store_true\", help=\"Use automatic mixed precision.\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def prepare_output_dir(output_dir: Path) -> None:\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (output_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
        "    (output_dir / \"metrics\").mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def instantiate_model(name: str, image_size: Tuple[int, int]) -> nn.Module:\n",
        "    model_cls = MODEL_MAP[name]\n",
        "    if name == \"transunet\":\n",
        "        return model_cls(img_size=image_size)\n",
        "    return model_cls()\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: torch.utils.data.DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scaler: Optional[amp.GradScaler],\n",
        "    device: torch.device,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    metrics_sum = {\"dice\": 0.0, \"iou\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        images = batch[\"image\"].to(device)\n",
        "        masks = batch[\"mask\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with amp.autocast(enabled=scaler is not None):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_metrics = compute_metrics(outputs.detach(), masks)\n",
        "        running_loss += loss.item()\n",
        "        for key in metrics_sum:\n",
        "            metrics_sum[key] += float(batch_metrics[key])\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = running_loss / max(1, num_batches)\n",
        "    avg_metrics = {k: v / max(1, num_batches) for k, v in metrics_sum.items()}\n",
        "    return avg_loss, avg_metrics\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    loader: torch.utils.data.DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    metrics_sum = {\"dice\": 0.0, \"iou\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n",
        "    num_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Eval\", leave=False):\n",
        "            images = batch[\"image\"].to(device)\n",
        "            masks = batch[\"mask\"].to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            batch_metrics = compute_metrics(outputs, masks)\n",
        "            running_loss += loss.item()\n",
        "            for key in metrics_sum:\n",
        "                metrics_sum[key] += float(batch_metrics[key])\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_loss = running_loss / max(1, num_batches)\n",
        "    avg_metrics = {k: v / max(1, num_batches) for k, v in metrics_sum.items()}\n",
        "    return avg_loss, avg_metrics\n",
        "\n",
        "\n",
        "def fit_model(\n",
        "    model_name: str,\n",
        "    args: argparse.Namespace,\n",
        "    datasets: Tuple[\n",
        "        torch.utils.data.Dataset,\n",
        "        torch.utils.data.Dataset,\n",
        "        torch.utils.data.Dataset,\n",
        "    ],\n",
        ") -> Dict[str, float]:\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(\n",
        "        datasets,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "    )\n",
        "    device = torch.device(args.device)\n",
        "    model = instantiate_model(model_name, tuple(args.image_size)).to(device)\n",
        "    criterion = LOSS_MAP[args.loss]\n",
        "    optimizer = Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=1e-5)\n",
        "    scaler = amp.GradScaler() if args.amp and device.type == \"cuda\" else None\n",
        "\n",
        "    best_val_dice = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    history: List[Dict[str, float]] = []\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        print()\n",
        "        print(f\"Epoch {epoch}/{args.epochs} - Model: {model_name}\")\n",
        "        train_loss, train_metrics = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device\n",
        "        )\n",
        "        val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        log_entry = {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            **{f\"train_{k}\": v for k, v in train_metrics.items()},\n",
        "            \"val_loss\": val_loss,\n",
        "            **{f\"val_{k}\": v for k, v in val_metrics.items()},\n",
        "        }\n",
        "        history.append(log_entry)\n",
        "        print(json.dumps(log_entry, indent=2))\n",
        "\n",
        "        current_val_dice = val_metrics[\"dice\"]\n",
        "        if current_val_dice > best_val_dice + 1e-4:\n",
        "            best_val_dice = current_val_dice\n",
        "            epochs_no_improve = 0\n",
        "            checkpoint_path = (\n",
        "                Path(args.output_dir)\n",
        "                / \"checkpoints\"\n",
        "                / f\"{model_name}_best.pth\"\n",
        "            )\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Saved new best checkpoint to {checkpoint_path}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= args.patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Load best checkpoint for testing\n",
        "    best_path = Path(args.output_dir) / \"checkpoints\" / f\"{model_name}_best.pth\"\n",
        "    if best_path.exists():\n",
        "        model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "        print(f\"Loaded best checkpoint from {best_path}\")\n",
        "\n",
        "    test_loss, test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "    print(\"Test metrics:\", test_metrics)\n",
        "\n",
        "    metrics_path = (\n",
        "        Path(args.output_dir) / \"metrics\" / f\"{model_name}_metrics.json\"\n",
        "    )\n",
        "    with metrics_path.open(\"w\") as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"history\": history,\n",
        "                \"best_val_dice\": best_val_dice,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_metrics\": {k: float(v) for k, v in test_metrics.items()},\n",
        "            },\n",
        "            f,\n",
        "            indent=2,\n",
        "        )\n",
        "    return {k: float(v) for k, v in test_metrics.items()}\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    seed_everything(args.seed)\n",
        "    output_dir = Path(args.output_dir)\n",
        "    prepare_output_dir(output_dir)\n",
        "\n",
        "    data_dir = resolve_data_dir(args.data_dir)\n",
        "\n",
        "    datasets = create_datasets(\n",
        "        data_dir,\n",
        "        image_size=tuple(args.image_size),\n",
        "        val_size=0.15,\n",
        "        test_size=0.15,\n",
        "        seed=args.seed,\n",
        "        augment=True,\n",
        "    )\n",
        "\n",
        "    summary: Dict[str, Dict[str, float]] = {}\n",
        "    for model_name in args.models:\n",
        "        metrics = fit_model(model_name, args, datasets)\n",
        "        summary[model_name] = metrics\n",
        "\n",
        "    summary_path = output_dir / \"metrics\" / \"summary.json\"\n",
        "    with summary_path.open(\"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(\"Saved summary metrics to\", summary_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "''',\n",
        "    \"src/plot_results.py\": '''\"\"\"\n",
        "Utility to aggregate and plot model comparison metrics.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_metrics(metrics_dir: Path) -> pd.DataFrame:\n",
        "    rows: List[Dict[str, float | str]] = []\n",
        "    for metrics_file in metrics_dir.glob(\"*_metrics.json\"):\n",
        "        with metrics_file.open() as f:\n",
        "            data = json.load(f)\n",
        "        model_name = metrics_file.stem.replace(\"_metrics\", \"\")\n",
        "        test_metrics = data.get(\"test_metrics\", {})\n",
        "        rows.append(\n",
        "            {\n",
        "                \"model\": model_name,\n",
        "                **test_metrics,\n",
        "            }\n",
        "        )\n",
        "    if not rows:\n",
        "        raise FileNotFoundError(f\"No metric files found in {metrics_dir}\")\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def plot_metrics(df: pd.DataFrame, output_dir: Path) -> None:\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for metric in [\"dice\", \"iou\", \"precision\", \"recall\"]:\n",
        "        if metric not in df.columns:\n",
        "            continue\n",
        "        ax = df.plot(\n",
        "            x=\"model\",\n",
        "            y=metric,\n",
        "            kind=\"bar\",\n",
        "            legend=False,\n",
        "            rot=0,\n",
        "            title=f\"{metric.upper()} comparison\",\n",
        "        )\n",
        "        ax.set_ylabel(metric.UPPER())\n",
        "        ax.set_ylim(0, 1.0)\n",
        "        plt.tight_layout()\n",
        "        figure_path = output_dir / f\"{metric}_comparison.png\"\n",
        "        plt.savefig(figure_path)\n",
        "        plt.close()\n",
        "        print(f\"Saved {metric} plot to {figure_path}\")\n",
        "\n",
        "\n",
        "def save_table(df: pd.DataFrame, output_dir: Path) -> None:\n",
        "    csv_path = output_dir / \"metrics_summary.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Saved metrics table to {csv_path}\")\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=\"Plot segmentation metric comparisons.\")\n",
        "    parser.add_argument(\n",
        "        \"--metrics_dir\",\n",
        "        type=str,\n",
        "        default=\"results/metrics\",\n",
        "        help=\"Directory containing *_metrics.json files.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        type=str,\n",
        "        default=\"results/plots\",\n",
        "        help=\"Directory to store generated figures and tables.\",\n",
        "    )\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    metrics_dir = Path(args.metrics_dir)\n",
        "    df = load_metrics(metrics_dir)\n",
        "    output_dir = Path(args.output_dir)\n",
        "    save_table(df, output_dir)\n",
        "    plot_metrics(df, output_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "'''\n",
        "}\n",
        "\n",
        "for relative_path, content in files.items():\n",
        "    destination = PROJECT_ROOT / relative_path\n",
        "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
        "    destination.write_text(content.rstrip(\"\\n\") + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Wrote {len(files)} files under {PROJECT_ROOT}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_file = PROJECT_ROOT / \"src/plot_results.py\"\n",
        "text = plot_file.read_text(encoding=\"utf-8\")\n",
        "text = text.replace(\"metric.UPPER()\", \"metric.upper()\")\n",
        "plot_file.write_text(text, encoding=\"utf-8\")\n",
        "print(\"Patched plot_results.py\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%cd /kaggle/working/angiogram-segmentation\n",
        "!pip install -r requirements.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configure dataset + hyperparameters\n",
        "Update the values below if you renamed the attached dataset or want to tweak training behaviour. By default, the scripts auto-discover `Database_134_Angiograms` anywhere under `/kaggle/input`. Results are written to `/kaggle/working/results` so they persist when you save a Notebook version.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    data_dir: str = \"Database_134_Angiograms\"\n",
        "    output_dir: str = \"/kaggle/working/results\"\n",
        "    image_size: tuple[int, int] = (512, 512)\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 8\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_workers: int = 2\n",
        "    models: tuple[str, ...] = (\"unetpp\", \"unet3plus\", \"transunet\")\n",
        "    loss: str = \"bce_dice\"\n",
        "    seed: int = 42\n",
        "\n",
        "CFG = TrainConfig()\n",
        "CFG\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocess + export optional `.npy` tensors\n",
        "This step creates train/val/test splits and (optionally) exports resized arrays for faster experimentation. Skip `--export` if you only need split definitions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "preprocess_cmd = [\n",
        "    \"python\",\n",
        "    \"-m\",\n",
        "    \"src.preprocess_dataset\",\n",
        "    \"--data_dir\",\n",
        "    CFG.data_dir,\n",
        "    \"--output_dir\",\n",
        "    str(Path(CFG.output_dir) / \"preprocessed\"),\n",
        "    \"--image_size\",\n",
        "    str(CFG.image_size[0]),\n",
        "    str(CFG.image_size[1]),\n",
        "    \"--export\",\n",
        "    \"--format\",\n",
        "    \"npy\",\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(preprocess_cmd))\n",
        "subprocess.run(preprocess_cmd, cwd=str(PROJECT_ROOT), check=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train UNet++, UNet 3+, TransUNet\n",
        "This cell sequentially trains all three architectures with shared hyperparameters from `CFG`. Adjust `CFG.models` if you only need a subset. Mixed precision (`--amp`) is enabled automatically when a GPU is available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_cmd = [\n",
        "    \"python\",\n",
        "    \"-m\",\n",
        "    \"src.train\",\n",
        "    \"--data_dir\",\n",
        "    CFG.data_dir,\n",
        "    \"--output_dir\",\n",
        "    CFG.output_dir,\n",
        "    \"--image_size\",\n",
        "    str(CFG.image_size[0]),\n",
        "    str(CFG.image_size[1]),\n",
        "    \"--epochs\",\n",
        "    str(CFG.epochs),\n",
        "    \"--batch_size\",\n",
        "    str(CFG.batch_size),\n",
        "    \"--learning_rate\",\n",
        "    str(CFG.learning_rate),\n",
        "    \"--weight_decay\",\n",
        "    str(CFG.weight_decay),\n",
        "    \"--num_workers\",\n",
        "    str(CFG.num_workers),\n",
        "    \"--loss\",\n",
        "    CFG.loss,\n",
        "    \"--seed\",\n",
        "    str(CFG.seed),\n",
        "    \"--models\",\n",
        "    *CFG.models,\n",
        "    \"--amp\",\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(train_cmd))\n",
        "subprocess.run(train_cmd, cwd=str(PROJECT_ROOT), check=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Aggregate metrics + plots\n",
        "Generates `metrics_summary.csv` plus per-metric bar charts under `/kaggle/working/results/plots`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_cmd = [\n",
        "    \"python\",\n",
        "    \"-m\",\n",
        "    \"src.plot_results\",\n",
        "    \"--metrics_dir\",\n",
        "    str(Path(CFG.output_dir) / \"metrics\"),\n",
        "    \"--output_dir\",\n",
        "    str(Path(CFG.output_dir) / \"plots\"),\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(plot_cmd))\n",
        "subprocess.run(plot_cmd, cwd=str(PROJECT_ROOT), check=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quick glance at metrics + artifacts\n",
        "The snippet below prints the summary JSON, shows the metrics table, and lists which files were generated so you can download or publish them as a Kaggle Dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "summary_path = Path(CFG.output_dir) / \"metrics\" / \"summary.json\"\n",
        "if summary_path.exists():\n",
        "    summary = json.loads(summary_path.read_text())\n",
        "    print(\"Summary metrics:\\n\", json.dumps(summary, indent=2))\n",
        "else:\n",
        "    print(\"Summary file not found:\", summary_path)\n",
        "\n",
        "summary_csv = Path(CFG.output_dir) / \"plots\" / \"metrics_summary.csv\"\n",
        "if summary_csv.exists():\n",
        "    display(pd.read_csv(summary_csv))\n",
        "else:\n",
        "    print(\"metrics_summary.csv not found yet\")\n",
        "\n",
        "print(\"\\nArtifacts under\", CFG.output_dir)\n",
        "for path in sorted(Path(CFG.output_dir).rglob(\"*\")):\n",
        "    if path.is_file():\n",
        "        print(\"-\", path.relative_to(Path(CFG.output_dir)))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
